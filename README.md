# Electronix AI - Sentiment Analysis Microservice

This project is an end-to-end, containerized microservice for binary sentiment analysis. It consists of a Python backend that serves a Hugging Face Transformer model, a standalone script for fine-tuning, and a minimal React frontend to display the results.

---
## Vidoe demonstration: https://youtu.be/1_EXuoJUsjc
## Tech Stack

-   **Backend**: Python, FastAPI, Hugging Face `transformers`, PyTorch
-   **Frontend**: React (with Vite), TailwindCSS, ShadCN, Nginx (for serving production build)
-   **Containerization**: Docker, Docker Compose

---

## Setup & Run Instructions

### Prerequisites

-   Docker
-   Docker Compose

### 1. (Optional) Fine-Tuning the Model

The application can load a pre-trained model by default. However, you can fine-tune it on your own data using the provided script.

1.  Prepare your data in a `data.jsonl` file inside the `./backend` directory. Each line should be a JSON object, for example:
    ```json
    {"text": "This is a great product!", "label": "positive"}
    ```
2.  Navigate to the backend directory and create a virtual env then run the fine-tuning script. You can adjust the epochs and learning rate as needed.
    ```bash
    cd backend
    python -m venv myenv
    source myenv/bin/activate
    pip install -r requirements.txt
    python finetune.py --data data.jsonl --epochs 3 --lr 3e-5
    ```
    This saves the updated model weights to the `./backend/model` directory, which will be automatically loaded by the API on the next startup.

### 2. Running the Application

The entire project can be built and run with a single command from the root directory.

```bash
docker compose up --build
```



# Design Decisions



## REST API over GraphQL

- The project uses a **standard REST API**.
- **Reasoning**:  
  REST is simpler to implement for a **single-purpose application** and is **widely understood** by developers.
- Since the service provides just **one prediction endpoint**, using GraphQL would have added unnecessary complexity.

---

## FastAPI as the Backend Framework

- The backend is built using **FastAPI**, a modern and high-performance web framework for Python.
- **Key Benefits**:
  - **Speed**: Extremely fast, thanks to asynchronous support.
  - **Interactive Documentation**: Automatically generates Swagger UI and OpenAPI docs.
  - **Pydantic Integration**: Simplifies request and response data validation using Python type hints.

---

## Standard Hugging Face Model as Default

- The default model used is:  
  **`distilbert-base-uncased-finetuned-sst-2-english`**
- **Why this model?**:
  - Balances **speed and accuracy** well.
  - Ideal for **sentiment analysis**.
  - Works **out of the box** without requiring fine-tuning.

---

## Separation of Training and Inference

- The **fine-tuning script** is a separate **command-line tool**.
- **Purpose**:  
  Keeps the API **lightweight** by offloading heavy training logic.
- **Inference Only**:
  - The API handles only prediction (inference).
  - Once training is complete, the API loads the **trained model files** during startup.

---

## üê≥ Docker Compose for Simplicity

- The project uses **Docker Compose** to manage services.
- **Why Docker Compose?**:
  - Manages **both backend (FastAPI)** and **frontend (React)** together.
  - **Single-command startup**:  
    ```bash
    docker compose up --build
    ```
  - Simplifies **local development and deployment**.

---

This architecture ensures the application is **modular**, **scalable**, and **easy to set up and use**.


# API Documentation

Interactive API documentation is automatically generated by FastAPI and is available at:

**[http://localhost:8000/docs](http://localhost:8000/docs)**

---

## üìç Predict Endpoint

- **URL**: `/predict`  
- **Method**: `POST`  

### üì§ Request Body (JSON)

```json
{
  "text": "Your text for analysis here"
}
```
### Success Response (200 OK)

```json
{
    "label": "positive",
    "score": 0.9998
}
```
The label will be either `"positive"` or `"negative"`.

